# Today I Learned

## March 21, 2023

- Classification: K-Nearest Neighbors (KNN) algorithm
  - Learned how to build a KNN model to make predictions and how to access the model's accuracy
  - Clarified the concepts of underfitting and overfitting in machine learning
  - Discovered an effective way to see the change of accuracy according to the number of neighbors by using a for loop

## March 22, 2023

- Git
  - I was supposed to take a lesson regarding regression.
  - Instead, today I learned how to use Git to manage version control for my code projects.
  - It took some time and effort, but I successfully connected Git and GitHub, and also integrated Git with my Jupyter Notebook workflow.
  - I am writing this on Jupyter notebook, and it will be automatically in my GitHub page. Amazing!
  
## March 23, 2023

- Regression
  - I learned that in scikit-learn, features must be represented as a two-dimensional array.
  - I figured out that if I have a one-dimensional feature vector, I can convert it into a two-dimensional feature matrix with one column using NumPy's reshape() method, which is the required format for scikit-learn.

## March 24, 2023

- Even though I was tired from a job interview earlier today, I was determined to dedicate my time to learning regression models.
- Regression
  - During my review of regression models, I focused on two common evaluation metrics: R-squared (R^2) and Root Mean Squared Error (RMSE).
  - At first, I was confused about how to calculate these metrics using Python. However, I was able to figure it out and now have a better understanding of how to use libraries like sklearn.metrics and numpy to compute R^2 and RMSE for regression models.
  
## March 25, 2023

- Cross-validation
  - I learned how to run cross-validation on Python using sklearn.
  - When I was taking an advanced regression analysis course, this was one of the hardest concepts for me to understand.
  - However, now I realize it is quite simple to implement and I understand what it's for.

- Regularized regression: Ridge & Lasso
  - I ran Ridge and Lasso, which are forms of regularized regression.
  - These were also concepts I struggled to understand, especially the formulas.
  - However, upon reviewing them, I found them to be quite simple and I understood the difference between them.
  - Specifically, Lasso has some functions that allow for variable selection, unlike Ridge.